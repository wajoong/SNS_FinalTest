{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.크롤링할 키워드는 무엇입니까?(예:여행): 여행\n",
      "2.포함할 키워드를 입력하세요.(예:환전,면세):환전\n",
      "3.제외할 키워드를 입력하세요.(예:중국,미국):중국\n",
      "4.조회를 시작할 날짜를 입력하세요.(예:20200101):20200101\n",
      "5.조회를 종료할 날짜를 입력하세요.(예:20201231):20201231\n",
      "6.크롤링할 건수는 몇건입니까?:28\n",
      "7.결과 파일을 저장할 폴더명을 쓰세요(예:c:\\data\\):C:\\Users\\UIT801-\\Desktop\\20191469\\3학년 1학기\\SNS빅데이터\\과제\\SNS_기말과제\\Q2\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UIT801-\\AppData\\Local\\Temp\\ipykernel_17116\\3906059191.py:32: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "요청하신 데이터 추출이 성공적으로 끝났습니다.\n"
     ]
    }
   ],
   "source": [
    "#Step 1. 필요한 모듈과 라이브러리를 import 합니다\n",
    "\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy\n",
    "import xlwt\n",
    "\n",
    "keyword = input('1.크롤링할 키워드는 무엇입니까?(예:여행): ')\n",
    "in_ = list(input('2.포함할 키워드를 입력하세요.(예:환전,면세):').split(','))\n",
    "ex_ = list(input('3.제외할 키워드를 입력하세요.(예:중국,미국):').split(','))\n",
    "start_d = input('4.조회를 시작할 날짜를 입력하세요.(예:20200101):')\n",
    "end_d = input('5.조회를 종료할 날짜를 입력하세요.(예:20201231):')\n",
    "how_many = int(input('6.크롤링할 건수는 몇건입니까?:'))\n",
    "f_dir = input('7.결과 파일을 저장할 폴더명을 쓰세요(예:c:\\\\data\\\\):')\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "os.makedirs(f_dir+s+'-'+keyword)\n",
    "os.chdir(f_dir+s+'-'+keyword)\n",
    "f_txt=f_dir+s+'-'+keyword+'\\\\'+s+'-'+keyword+'.txt'\n",
    "f_csv=f_dir+s+'-'+keyword+'\\\\'+s+'-'+keyword+'.csv'\n",
    "f_xls=f_dir+s+'-'+keyword+'\\\\'+s+'-'+keyword+'.xlsx'\n",
    "path = \"C:\\Temp\\chromedriver_win32\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"https://www.naver.com\")\n",
    "time.sleep(1)\n",
    "\n",
    "url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=\" + keyword\n",
    "url = url + '+%2B' + '+%2B'.join(in_) + '+-' + '+-'.join(ex_) + \"&sm=tab_opt&nso=p%3Afrom\" + start_d + \"to\" + end_d\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "driver.find_element(By.LINK_TEXT,\"VIEW\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.LINK_TEXT,\"블로그\").click()\n",
    "\n",
    "for _ in range(int(how_many / 30)):\n",
    "    body = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "    body.send_keys(Keys.END)\n",
    "    time.sleep(3)\n",
    "n=1\n",
    "n2 = 1\n",
    "nl = []\n",
    "urls = []\n",
    "titles = []\n",
    "nicks = []\n",
    "dates = []\n",
    "cons = []\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "content_list = soup.find('ul', class_='lst_total')\n",
    "\n",
    "for i in content_list.find_all('li', \"bx\"):\n",
    "    if n <= how_many:\n",
    "        url = i.find('a', 'api_txt_lines total_tit').get_text()\n",
    "        url = i.find('a', 'api_txt_lines total_tit').get(\"href\")\n",
    "        urls.append(url)\n",
    "        n += 1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        driver.switch_to.frame('mainFrame')\n",
    "    except:\n",
    "        print(\"\")\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    nl.append(n2)\n",
    "    n2 += 1\n",
    "    title = soup.find('div', 'pcol1')\n",
    "    if title is None:\n",
    "        title = soup.find('div', 'htitle')\n",
    "    if title is None:\n",
    "        title = soup.find('div', 'titleWrap jumbotron')\n",
    "    try:\n",
    "        title = title.get_text().replace(\"\\n\",\"\")\n",
    "    except:\n",
    "        title = \"추출실패\"\n",
    "    titles.append(title)\n",
    "\n",
    "    nick = soup.find('span', 'nick')\n",
    "    if nick is None:\n",
    "        nick = soup.find('div', 'nick')\n",
    "    try:\n",
    "        nick = nick.get_text()\n",
    "    except:\n",
    "        nick = \"추출실패\"\n",
    "    nicks.append(nick)\n",
    "\n",
    "    date = soup.find('span', 'se_publishDate pcol2')\n",
    "    if date is None:\n",
    "        date = soup.find('p', 'date fil5 pcol2 _postAddDate')\n",
    "    try:\n",
    "        date = date.get_text()\n",
    "    except:\n",
    "        date = \"추출실패\"\n",
    "    dates.append(date)\n",
    "\n",
    "    con = soup.find('div', 'se-main-container')\n",
    "    if con is None:\n",
    "        con = soup.find('div', id='postViewArea')\n",
    "    if con is None:\n",
    "        con = soup.find('div', 'se_component_wrap sect_dsc __se_component_area')\n",
    "    try:\n",
    "        con = con.get_text().replace(\"\\n\",\"\")\n",
    "    except:\n",
    "        con = \"추출실패\"\n",
    "    cons.append(con)\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "df['제목'] = titles\n",
    "df['주소'] = urls\n",
    "df['작성자'] = nicks\n",
    "df['작성일'] = dates\n",
    "df['내용'] = cons\n",
    "df.to_excel(f_xls)\n",
    "df.to_csv(f_csv, encoding=\"utf-8-sig\")       \n",
    "f = open(f_txt, 'a',encoding='UTF-8')\n",
    "for i in range(len(nl)):\n",
    "    f.write('총 '+ str(how_many) + '건중 ' + str(nl[i]) + '번째 블로그 데이터를 수집합니다. ~~\\n')\n",
    "    f.write('1.제목: ' + titles[i] + '\\n')\n",
    "    f.write('2.주소: ' + urls[i] + '\\n')\n",
    "    f.write('3.작성자: ' + nicks[i] + '\\n')\n",
    "    f.write('4.작성일: ' + dates[i] + '\\n')\n",
    "    f.write('5.내용: ' + cons[i] + '\\n\\n')\n",
    "f.close()\n",
    "print(\"요청하신 데이터 추출이 성공적으로 끝났습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
